{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gdelt\n",
    "import pandas as pd\n",
    "import numpy as  np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer,OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer,mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:69: UserWarning: GDELT does not have a url for date time 20171001234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:105: UserWarning: GDELT did not return data for date time 20171001234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:69: UserWarning: GDELT does not have a url for date time 20171030234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:105: UserWarning: GDELT did not return data for date time 20171030234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:69: UserWarning: GDELT does not have a url for date time 20171130234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:105: UserWarning: GDELT did not return data for date time 20171130234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:69: UserWarning: GDELT does not have a url for date time 20171202234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:105: UserWarning: GDELT did not return data for date time 20171202234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:69: UserWarning: GDELT does not have a url for date time 20171215234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:105: UserWarning: GDELT did not return data for date time 20171215234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:69: UserWarning: GDELT does not have a url for date time 20180102234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:105: UserWarning: GDELT did not return data for date time 20180102234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:69: UserWarning: GDELT does not have a url for date time 20180109234500\n",
      "  warnings.warn(message)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/gdelt/parallel.py:105: UserWarning: GDELT did not return data for date time 20180109234500\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "gd = gdelt.gdelt()\n",
    "\n",
    "df=gd.Search(['2017 10 01','2018 02 18'],table='events')\n",
    "df = df[df.EventRootCode != '--']\n",
    "df.index = df.SQLDATE\n",
    "df.sort_index(inplace=True)\n",
    "df.index = pd.to_datetime(df.index, format='%Y%m%d')\n",
    "df.drop(['SQLDATE','QuadClass'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213439, 60)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(df,country,code):\n",
    "    scaler = MinMaxScaler(feature_range=(0,50))\n",
    "    df.NumMentions = scaler.fit_transform(df['NumMentions'].reshape(-1,1))\n",
    "    df = df[df.ActionGeo_CountryCode == country]\n",
    "    indices = df.index.unique()\n",
    "    aggregated_df = pd.DataFrame(columns=['ActionGeo_CountryCode', 'EventRootCode', 'NumMentions','GoldsteinScale', 'AvgTone'], index= indices)\n",
    "    aggregated_df.ActionGeo_CountryCode = country\n",
    "    aggregated_df.EventRootCode = code\n",
    "    for i in range(len(indices)):\n",
    "        aggregated_df.NumMentions[indices[i]] = df.NumMentions[indices[i]].sum()\n",
    "        aggregated_df.GoldsteinScale[indices[i]] = df.GoldsteinScale[indices[i]].max()\n",
    "        aggregated_df.AvgTone[indices[i]] = df.AvgTone[indices[i]].mean()\n",
    "    \n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data(df,lag_days):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(df)-lag_days-1):\n",
    "        a = df[i:(i+lag_days)]\n",
    "        dataX.append(np.array(a))\n",
    "        dataY.append(y_df.iloc[i+lag_days])\n",
    "    \n",
    "    return np.array(dataX), np.array(dataY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def specificCountry_orderedEvents(data,country,days,plot=False):\n",
    "    z = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "    df = pd.DataFrame(columns=z)\n",
    "    l = []\n",
    "    for i in z:\n",
    "        l.append(get_data(data,country, i).NumMentions)\n",
    "    for i in z:\n",
    "        df[i] = l[i-1]\n",
    "    plot_list = []\n",
    "    df = df.fillna(0)\n",
    "    df = df[:(len(df)-len(df)%days)]\n",
    "    for i in range(int( df.shape[0]/days )):\n",
    "        plot_list.append(df[days*i:days*(i+1)].sum())\n",
    "    if(plot):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.set_yticks([25*i for i in range(11)])\n",
    "        ax.set_xticks(z)\n",
    "        plt.bar(z,plot_list[52])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    \n",
    "    \n",
    "    #''''''''''''''''''''''''''''''''''''''\n",
    "    # data for protest for all country    '\n",
    "    #''''''''''''''''''''''''''''''''''''''\n",
    "    \n",
    "    core_df =df\n",
    "    core_df.ActionGeo_CountryCode.dropna(inplace=True)\n",
    "    country_list = core_df.ActionGeo_CountryCode.unique()\n",
    "    code = 14\n",
    "    all_country_timeseries = {}\n",
    "    for country in country_list:\n",
    "        all_country_timeseries[country] = get_data(core_df,country,code).NumMentions\n",
    "        #print(\"For \"+country+\": Done\")\n",
    "    \n",
    "    df = pd.DataFrame(index=all_country_timeseries['US'].index, columns=country_list)\n",
    "    for country in country_list:\n",
    "        df[country] = all_country_timeseries[country]\n",
    "        \n",
    "    #'''''''''''''''''''''''''''''''\n",
    "    # data for US for all event    '\n",
    "    #'''''''''''''''''''''''''''''''\n",
    "    \n",
    "    df_allEvent = specificCountry_orderedEvents(core_df,'US', 500)\n",
    "    str_column = df_allEvent.columns.astype(str)\n",
    "    df_allEvent.columns = str_column\n",
    "    \n",
    "    return df, df_allEvent, country_list, str_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import scipy.cluster.hierarchy\n",
    "\n",
    "\n",
    "from random import randint, seed\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def dtw(x, y):\n",
    "    dist, _ = fastdtw(x, y, dist=euclidean)\n",
    "    return dist\n",
    "\n",
    "def calculate_dtw_distances(dataframe):\n",
    "    n = len(list(dataframe))\n",
    "\n",
    "    dtw_distances = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            \n",
    "            # ADD PROVISION FOR NUMBER OF DAYS ( 500)\n",
    "            dtw_distances[i,j] = dtw(dataframe.ix[-500:,i].fillna(0).values, dataframe.ix[-500:,j].fillna(0).values)\n",
    "    \n",
    "        print(\"Done [\",i,\"]\")\n",
    "    return dtw_distances    \n",
    "\n",
    "\n",
    "# ADD PARAMETER FOR THE TYPE OF CLUSTERING \n",
    "def Cluster(number_of_clusters, dtw_distances ):\n",
    "    #NUMBER OF CLUSTERS\n",
    "    k = number_of_clusters\n",
    "\n",
    "    #K-means\n",
    "    y_pred_km_dtw = KMeans(n_clusters=k).fit_predict(dtw_distances)\n",
    "    \n",
    "    #Hierarchical\n",
    "    y_pred_hac_dtw = AgglomerativeClustering(n_clusters=k).fit_predict(dtw_distances)  \n",
    "    \n",
    "    return y_pred_hac_dtw, y_pred_km_dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_two(df, country_list):\n",
    "    try:\n",
    "        distance_matrix = np.load('dtw_distance.npy')\n",
    "    except:\n",
    "        distance_matrix = calculate_dtw_distances(df)\n",
    "        np.save('dtw_distance', distance_matrix)\n",
    "    \n",
    "    #print('Distance matrix calculated')\n",
    "    hac_all_country, kmeans_all_country = Cluster(6, distance_matrix)\n",
    "    country_cluster = {}\n",
    "    i = 0\n",
    "    for country in country_list:\n",
    "        country_cluster[country] = hac_all_country[i]\n",
    "        i += 1\n",
    "    #print(\"Cluster of countries: \", country_cluster)\n",
    "    \n",
    "    return country_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similar_country_data(data):\n",
    "    df_allCountry, df_allEvent, country_list, cameoCodes = load_data(data)\n",
    "    df_allCountry.fillna(0, inplace=True)\n",
    "    df_allEvent.fillna(0, inplace=True)\n",
    "    country_cluster = cluster_two(df_allCountry, country_list)\n",
    "\n",
    "    #get similar country\n",
    "    similar_country = []\n",
    "    CountryClusterValue = country_cluster[country]\n",
    "    for cntry in country_list:\n",
    "        try:\n",
    "            if country_cluster[cntry] == CountryClusterValue:\n",
    "                similar_country.append(cntry)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #get data for similar countries\n",
    "    #Fill missing date values\n",
    "    print()\n",
    "    print(\"Preparing dataset for similar countries: \", similar_country)\n",
    "    allSimilarCountryData = []\n",
    "    for cntry in similar_country:\n",
    "        df = get_data(data,cntry, code)\n",
    "        df = df.NumMentions\n",
    "        allSimilarCountryData.append(df)\n",
    "        \n",
    "    return allSimilarCountryData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allSimilarCountryLSTM(allSimilarCountryData, plot=True):\n",
    "    allSimilarCountryData = np.array(allSimilarCountryData)\n",
    "    allSimilarCountryData = allSimilarCountryData.transpose()\n",
    "    allSimilarCountryData = allSimilarCountryData.tolist()\n",
    "    \n",
    "    df = allSimilarCountryData\n",
    "    total_days = len(df)\n",
    "    \n",
    "    train_len = int(0.75*total_days)\n",
    "    train = df[:train_len]\n",
    "    test = df[train_len:]\n",
    "    \n",
    "    lag_days = 6\n",
    "    X_train, y_train = create_data(train, lag_days)\n",
    "    X_test, y_test = create_data(test, lag_days)\n",
    "    \n",
    "    batch_size = 4\n",
    "    epochs = 50\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, batch_input_shape=(None, lag_days, X_train.shape[2])))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss= 'logcosh', optimizer= 'Adagrad')\n",
    "    history = model.fit(X_train,y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(X_test, y_test))\n",
    "    \n",
    "    trainPredict = model.predict(X_train)\n",
    "    testPredict = model.predict(X_test)\n",
    "    \n",
    "    if plot:\n",
    "        \n",
    "        plt.figure(figsize=(12,7))\n",
    "        plt.plot(history.history['loss'], c='r', label='train_loss')\n",
    "        plt.plot(history.history['val_loss'], c='g', label='validation_loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title('Epoch vs Loss curve')\n",
    "        plt.legend(loc='best')\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(12,7))\n",
    "        plt.plot(testPredict,  c='r', label=\"testPredict\")\n",
    "        plt.plot(y_test, label=\"test\")\n",
    "        plt.title('Actual vs Predicted test set')\n",
    "        plt.legend(loc='best')\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(12,7))\n",
    "        plt.plot(trainPredict, c='r',label=\"trainPredict\")\n",
    "        plt.plot(y_train, label=\"train\")\n",
    "        plt.title('Actual vs Predicted on training set')\n",
    "        plt.legend(loc='best')\n",
    "        \n",
    "        \n",
    "        train_NumMentions = df.avgNumMentions\n",
    "        X_actual_data, y_actual_data = create_data(df, lag_days)\n",
    "        predicted_whole_series = model.predict(X_actual_data)\n",
    "        \n",
    "        plt.figure(figsize=(12,7))\n",
    "        plt.plot(predicted_whole_series, c='g', label=\"Predicted whole series\")\n",
    "        plt.plot(train_NumMentions, c='r', label=\"Training series\")\n",
    "        plt.title('Actual training vs whole series as prediction')\n",
    "        plt.legend(loc='best')\n",
    "    \n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    y_train = y_train.reshape(y_train.shape[0],1)\n",
    "\n",
    "    print(\"r-sq(train): \",r2_score(y_train, model.predict(X_train)))\n",
    "    print(\"r-sq(test): \",r2_score(y_test, testPredict))\n",
    "    print(\"rmse(train) :\", mean_squared_error(y_train, model.predict(X_train)))\n",
    "    print(\"rmse(test) :\", mean_squared_error(y_test, testPredict))\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    return testPredict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiru/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/chiru/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "load_data(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
